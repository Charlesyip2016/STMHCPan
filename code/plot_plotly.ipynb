{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a06b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c525acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.offline import iplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6231b0",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c87313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8957dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = pd.read_csv('../pic/netmhcpan41_test_data.csv')\n",
    "compare=compare.sort_values(by='allele')\n",
    "compare_A = compare[:]\n",
    "tmp_df1 = pd.DataFrame({'HLA':compare_A['allele'].to_list(),'AUC':compare_A['transformer'].to_list()})\n",
    "tmp_df2 = pd.DataFrame({'HLA':compare_A['allele'].to_list(),'AUC':compare_A['st'].to_list()})\n",
    "tmp_df3 = pd.DataFrame({'HLA':compare_A['allele'].to_list(),'AUC':compare_A['st_lstm'].to_list()})\n",
    "tmp_df4 = pd.DataFrame({'HLA':compare_A['allele'].to_list(),'AUC':compare_A['st_cnn'].to_list()})\n",
    "df_cat = pd.concat([tmp_df2,tmp_df3,tmp_df4,tmp_df1])\n",
    "df_cat['Method'] = ['Star-Transformer']*36+ ['Star-Transformer-LSTM']*36 + ['Star-Transformer-CNN']*36 + ['Transformer Encoder']*36\n",
    "df_cat['Type'] = df_cat['HLA'].map(lambda x: 'HLA-'+x[0])\n",
    "\n",
    "\n",
    "fig = px.scatter(df_cat,  # 数据集\n",
    "             x=\"HLA\",  # x轴\n",
    "             y=\"AUC\",  # y轴\n",
    "             color=\"Method\",  # 指定颜色\n",
    "             symbol='Method',\n",
    "                 facet_col=\"Type\",\n",
    "                 color_discrete_sequence=px.colors.qualitative.Bold,\n",
    "                 facet_col_spacing=0.01\n",
    "            )\n",
    "fig.update_xaxes(tickangle=90, tickfont=dict(family='black', size=12))\n",
    "fig.update_xaxes(matches=None)\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))  \n",
    "\n",
    "for axis in fig.layout:\n",
    "    if type(fig.layout[axis]) == go.layout.XAxis:\n",
    "        fig.layout[axis].title.text = ''\n",
    "\n",
    "fig.update_traces(marker=dict(size=12),\n",
    "                  selector=dict(mode='markers'))\n",
    "fig.update_xaxes(showline=True, linewidth=1, linecolor='black', mirror=False)\n",
    "fig.update_yaxes(showline=True, linewidth=1, linecolor='black', mirror=False)\n",
    "\n",
    "fig.update_layout(width=1000,height=500,\n",
    "                      paper_bgcolor='rgba(0,0,0,0)',\n",
    "    plot_bgcolor='rgba(0,0,0,0)',\n",
    "        font=dict(\n",
    "        family=\"black\",\n",
    "        size=18))\n",
    "fig.update_layout(\n",
    "    legend=dict(\n",
    "        title_font_family=\"black\",  # 图例标题字体\n",
    "        font=dict(  # 图例字体\n",
    "            family=\"black\",\n",
    "            size=15,\n",
    "        ),\n",
    "        itemwidth=30,\n",
    "        tracegroupgap=0,\n",
    "        \n",
    "    )\n",
    ")\n",
    "fig.update_layout(legend=dict(\n",
    "    orientation=\"h\",  # 开启水平显示\n",
    "    yanchor=\"bottom\",\n",
    "    y=1.12,\n",
    "    xanchor=\"right\",\n",
    "    x=1\n",
    "))\n",
    "fig.update_layout(showlegend=True,   # 隐藏图例，默认是True\n",
    "                  legend_title_text=''   # 修改图例的名称\n",
    "                 )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f99a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da47acd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1102b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../pic/external_set_compare.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e62c7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(data, x=\"nt_f1\", y=\"st_f1\",\n",
    "                 color=\"HLA\", color_discrete_sequence=px.colors.qualitative.Dark24)\n",
    "fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = [0.5,1],\n",
    "            y = [0.5,1],\n",
    "            mode = \"lines\",\n",
    "            line = {'dash':'dash','color' : \"gray\"},\n",
    "            showlegend=False\n",
    "        )\n",
    ")\n",
    "fig.update_traces(marker=dict(size=12),\n",
    "                  selector=dict(mode='markers'))\n",
    "fig.update_xaxes(showline=True, linewidth=1, linecolor='black', mirror=False)\n",
    "fig.update_yaxes(showline=True, linewidth=1, linecolor='black', mirror=False)\n",
    "\n",
    "fig.update_layout(width=500,height=500,\n",
    "                      paper_bgcolor='rgba(0,0,0,0)',\n",
    "    plot_bgcolor='rgba(0,0,0,0)',\n",
    "        font=dict(\n",
    "        family=\"black\",\n",
    "        size=18))\n",
    "fig.update_traces(showlegend=False)\n",
    "fig.update_yaxes(range = [0.5,1])\n",
    "fig.update_xaxes(range = [0.5,1])\n",
    "fig.update_layout(\n",
    "    title=\"F1\",\n",
    "    xaxis_title=\"NetMHCpan4.1\",\n",
    "    yaxis_title=\"Star-Transformer\",\n",
    "    font=dict(\n",
    "        family=\"black\",\n",
    "        size=18,\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6717ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "iplot(fig, image='svg', filename='external_set_f1', image_width=500, image_height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143daf3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfea682",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_bar = ['#2E91E5',\n",
    " '#E15F99',\n",
    " '#1CA71C',\n",
    " '#FB0D0D',\n",
    " '#DA16FF',\n",
    " '#222A2A',\n",
    " '#B68100',\n",
    " '#750D86',\n",
    " '#EB663B',\n",
    " '#511CFB',\n",
    " '#00A08B',\n",
    " '#FB00D1',\n",
    " '#FC0080',\n",
    " '#B2828D',\n",
    " '#6C7C32',\n",
    " '#778AAE',\n",
    " '#862A16',\n",
    " '#A777F1',\n",
    " '#620042',\n",
    " '#1616A7',\n",
    " '#DA60CA',\n",
    " '#6C4516',\n",
    " '#0D2A63',\n",
    " '#AF0038',\n",
    "'#3366CC',\n",
    " '#DC3912',\n",
    " '#FF9900',\n",
    " '#109618',\n",
    " '#990099',\n",
    " '#0099C6',\n",
    " '#DD4477',\n",
    " '#66AA00',\n",
    " '#B82E2E',\n",
    " '#316395']\n",
    "def color(color, text):\n",
    "    return f\"<span style='color:{str(color)}'> {str(text)} </span>\"\n",
    "\n",
    "\n",
    "colors = color_bar[19:34]#[15:19]#['#FD3216', '#00FE35', '#6A76FC', '#FED4C4','#FE00CE','#0DF9FF','#F6F926',]\n",
    "ticks = y_list\n",
    "keys = dict(zip(ticks, colors))\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    y=y_list,\n",
    "    x=x_list_pos,\n",
    "    name='#Positives',\n",
    "    orientation='h',\n",
    "#     width=1,\n",
    "    marker=dict(\n",
    "        color='rgba(246, 78, 139, 0.6)',\n",
    "        line=dict(color='rgba(246, 78, 139, 1.0)', width=3)\n",
    "    )\n",
    "))\n",
    "fig.add_trace(go.Bar(\n",
    "    y=y_list,\n",
    "    x=x_list_neg,\n",
    "    name='#Negatives',\n",
    "    orientation='h',\n",
    "#     width=1,\n",
    "    marker=dict(\n",
    "        color='rgba(58, 71, 80, 0.6)',\n",
    "        line=dict(color='rgba(58, 71, 80, 1.0)', width=3)\n",
    "    )\n",
    "))\n",
    "\n",
    "\n",
    "fig.update_layout(width=600,height=600,\n",
    "                      paper_bgcolor='rgba(0,0,0,0)',\n",
    "    plot_bgcolor='rgba(0,0,0,0)',\n",
    "        font=dict(\n",
    "        family=\"black\",\n",
    "        size=18))\n",
    "fig.update_layout(legend=dict(\n",
    "    orientation=\"h\",  # 开启水平显示\n",
    "    yanchor=\"bottom\",\n",
    "    y=1.02,\n",
    "    xanchor=\"right\",\n",
    "    x=0.9,\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    font=dict(\n",
    "        family=\"black\",\n",
    "        size=18,\n",
    "    )\n",
    ")\n",
    "fig.update_xaxes(showline=True, linewidth=1, linecolor='black', mirror=False)\n",
    "fig.update_yaxes(showline=True, linewidth=1, linecolor='black', mirror=False)\n",
    "fig.update_layout(barmode='stack')#overlay #stack\n",
    "fig.update_layout(xaxis_type=\"log\")\n",
    "\n",
    "\n",
    "ticktext = [color(v, k) for k, v in keys.items()]\n",
    "print(ticktext)\n",
    "fig.update_layout(\n",
    "yaxis=dict(tickmode='array', ticktext=ticktext, tickvals=ticks)\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707959de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96cfde54",
   "metadata": {},
   "source": [
    "## Attention plot（ref: TranspHLA）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c59a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StarTransformer(nn.Module):\n",
    "    r\"\"\"\n",
    "    Star-Transformer 的encoder部分。 输入3d的文本输入, 返回相同长度的文本编码\n",
    "    paper: https://arxiv.org/abs/1902.09113\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, num_layers, num_head, head_dim, dropout=0.1, max_len=None):\n",
    "        r\"\"\"\n",
    "        \n",
    "        :param int hidden_size: 输入维度的大小。同时也是输出维度的大小。\n",
    "        :param int num_layers: star-transformer的层数\n",
    "        :param int num_head: head的数量。\n",
    "        :param int head_dim: 每个head的维度大小。\n",
    "        :param float dropout: dropout 概率. Default: 0.1\n",
    "        :param int max_len: int or None, 如果为int，输入序列的最大长度，\n",
    "            模型会为输入序列加上position embedding。\n",
    "            若为`None`，忽略加上position embedding的步骤. Default: `None`\n",
    "        \"\"\"\n",
    "        super(StarTransformer, self).__init__()\n",
    "        self.iters = num_layers\n",
    "\n",
    "        self.norm = nn.ModuleList([nn.LayerNorm(hidden_size, eps=1e-6) for _ in range(self.iters)])\n",
    "        # self.emb_fc = nn.Conv2d(hidden_size, hidden_size, 1)\n",
    "        self.emb_drop = nn.Dropout(dropout)\n",
    "        self.ring_att = nn.ModuleList(\n",
    "            [_MSA1(hidden_size, nhead=num_head, head_dim=head_dim, dropout=0.0)\n",
    "             for _ in range(self.iters)])\n",
    "        self.star_att = nn.ModuleList(\n",
    "            [_MSA2(hidden_size, nhead=num_head, head_dim=head_dim, dropout=0.0)\n",
    "             for _ in range(self.iters)])\n",
    "\n",
    "        if max_len is not None:\n",
    "            self.pos_emb = nn.Embedding(max_len, hidden_size)\n",
    "        else:\n",
    "            self.pos_emb = None\n",
    "\n",
    "    def forward(self, data, mask):\n",
    "        r\"\"\"\n",
    "        :param FloatTensor data: [batch, length, hidden] 输入的序列\n",
    "        :param ByteTensor mask: [batch, length] 输入序列的padding mask, 在没有内容(padding 部分) 为 0,\n",
    "            否则为 1\n",
    "        :return: [batch, length, hidden] 编码后的输出序列\n",
    "                [batch, hidden] 全局 relay 节点, 详见论文\n",
    "        \"\"\"\n",
    "\n",
    "        def norm_func(f, x):\n",
    "            # B, H, L, 1\n",
    "            return f(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
    "\n",
    "        B, L, H = data.size()\n",
    "        mask = (mask.eq(False))  # flip the mask for masked_fill_\n",
    "        smask = torch.cat([torch.zeros(B, 1, ).byte().to(mask), mask], 1)\n",
    "\n",
    "        embs = data.permute(0, 2, 1)[:, :, :, None]  # B H L 1\n",
    "        if self.pos_emb:\n",
    "            P = self.pos_emb(torch.arange(L, dtype=torch.long, device=embs.device) \\\n",
    "                             .view(1, L)).permute(0, 2, 1).contiguous()[:, :, :, None]  # 1 H L 1\n",
    "            embs = embs + P\n",
    "        embs = norm_func(self.emb_drop, embs)\n",
    "        nodes = embs\n",
    "        relay = embs.mean(2, keepdim=True)\n",
    "        ex_mask = mask[:, None, :, None].expand(B, H, L, 1)\n",
    "        r_embs = embs.view(B, H, 1, L)\n",
    "#         nodes_attns = []\n",
    "        relays_attns = []\n",
    "        for i in range(self.iters):\n",
    "            ax = torch.cat([r_embs, relay.expand(B, H, 1, L)], 2)\n",
    "            nodes, nodes_att = self.ring_att[i](norm_func(self.norm[i], nodes), ax=ax)\n",
    "            nodes = F.leaky_relu(nodes)\n",
    "            # nodes = F.leaky_relu(self.ring_att[i](nodes, ax=ax))\n",
    "            relay, relay_att = self.star_att[i](relay, torch.cat([relay, nodes], 2), smask)\n",
    "            relay = F.leaky_relu(relay)\n",
    "            relays_attns.append(relay_att)\n",
    "#             nodes_attns.append(nodes_att)\n",
    "            nodes = nodes.masked_fill_(ex_mask, 0)\n",
    "\n",
    "        nodes = nodes.view(B, H, L).permute(0, 2, 1)\n",
    "\n",
    "        return nodes, relay.view(B, H), relays_attns\n",
    "\n",
    "\n",
    "class _MSA1(nn.Module):\n",
    "    def __init__(self, nhid, nhead=10, head_dim=10, dropout=0.1):\n",
    "        super(_MSA1, self).__init__()\n",
    "        # Multi-head Self Attention Case 1, doing self-attention for small regions\n",
    "        # Due to the architecture of GPU, using hadamard production and summation are faster than dot production when unfold_size is very small\n",
    "        self.WQ = nn.Conv2d(nhid, nhead * head_dim, 1)\n",
    "        self.WK = nn.Conv2d(nhid, nhead * head_dim, 1)\n",
    "        self.WV = nn.Conv2d(nhid, nhead * head_dim, 1)\n",
    "        self.WO = nn.Conv2d(nhead * head_dim, nhid, 1)\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.nhid, self.nhead, self.head_dim, self.unfold_size = nhid, nhead, head_dim, 3\n",
    "        self.attn = None\n",
    "\n",
    "    def forward(self, x, ax=None):\n",
    "        # x: B, H, L, 1, ax : B, H, X, L append features\n",
    "        nhid, nhead, head_dim, unfold_size = self.nhid, self.nhead, self.head_dim, self.unfold_size\n",
    "        B, H, L, _ = x.shape\n",
    "        q, k, v = self.WQ(x), self.WK(x), self.WV(x)  # x: (B,H,L,1)\n",
    "\n",
    "        if ax is not None:\n",
    "            aL = ax.shape[2]\n",
    "            ak = self.WK(ax).view(B, nhead, head_dim, aL, L)\n",
    "            av = self.WV(ax).view(B, nhead, head_dim, aL, L)\n",
    "        q = q.view(B, nhead, head_dim, 1, L)\n",
    "        k = F.unfold(k.view(B, nhead * head_dim, L, 1), (unfold_size, 1), padding=(unfold_size // 2, 0)) \\\n",
    "            .view(B, nhead, head_dim, unfold_size, L)\n",
    "        v = F.unfold(v.view(B, nhead * head_dim, L, 1), (unfold_size, 1), padding=(unfold_size // 2, 0)) \\\n",
    "            .view(B, nhead, head_dim, unfold_size, L)\n",
    "        if ax is not None:\n",
    "            k = torch.cat([k, ak], 3)\n",
    "            v = torch.cat([v, av], 3)\n",
    "        alphas = self.drop(F.softmax((q * k).sum(2, keepdim=True) / np.sqrt(head_dim), 3))  # B N L 1 U\n",
    "        print('alphas shape',alphas.shape) #[1024, 8, 1, 5, 49]\n",
    "        att = (alphas * v).sum(3).view(B, nhead * head_dim, L, 1)\n",
    "        ret = self.WO(att)\n",
    "\n",
    "        return ret ,alphas\n",
    "\n",
    "\n",
    "class _MSA2(nn.Module):\n",
    "    def __init__(self, nhid, nhead=10, head_dim=10, dropout=0.1):\n",
    "        # Multi-head Self Attention Case 2, a broadcastable query for a sequence key and value\n",
    "        super(_MSA2, self).__init__()\n",
    "        self.WQ = nn.Conv2d(nhid, nhead * head_dim, 1)\n",
    "        self.WK = nn.Conv2d(nhid, nhead * head_dim, 1)\n",
    "        self.WV = nn.Conv2d(nhid, nhead * head_dim, 1)\n",
    "        self.WO = nn.Conv2d(nhead * head_dim, nhid, 1)\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.nhid, self.nhead, self.head_dim, self.unfold_size = nhid, nhead, head_dim, 3\n",
    "    def forward(self, x, y, mask=None):\n",
    "        # x: B, H, 1, 1, 1 y: B H L 1\n",
    "        nhid, nhead, head_dim, unfold_size = self.nhid, self.nhead, self.head_dim, self.unfold_size\n",
    "        B, H, L, _ = y.shape\n",
    "\n",
    "        q, k, v = self.WQ(x), self.WK(y), self.WV(y)\n",
    "\n",
    "        q = q.view(B, nhead, 1, head_dim)  # B, H, 1, 1 -> B, N, 1, h\n",
    "        k = k.view(B, nhead, head_dim, L)  # B, H, L, 1 -> B, N, h, L\n",
    "        v = v.view(B, nhead, head_dim, L).permute(0, 1, 3, 2)  # B, H, L, 1 -> B, N, L, h\n",
    "        pre_a = torch.matmul(q, k) / np.sqrt(head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            pre_a = pre_a.masked_fill(mask[:, None, None, :], -float('inf'))\n",
    "        alphas = self.drop(F.softmax(pre_a, 3))  # B, N, 1, L\n",
    "        att = torch.matmul(alphas, v).view(B, -1, 1, 1)  # B, N, 1, h -> B, N*h, 1, 1\n",
    "        return self.WO(att) ,alphas\n",
    "    \n",
    "class StarTransEnc(nn.Module):\n",
    "    r\"\"\"\n",
    "    带word embedding的Star-Transformer Encoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed,\n",
    "                 hidden_size,\n",
    "                 num_layers,\n",
    "                 num_head,\n",
    "                 head_dim,\n",
    "                 max_len,\n",
    "                 emb_dropout,\n",
    "                 dropout):\n",
    "        r\"\"\"\n",
    "        \n",
    "        :param embed: 单词词典, 可以是 tuple, 包括(num_embedings, embedding_dim), 即\n",
    "            embedding的大小和每个词的维度. 也可以传入 nn.Embedding 对象,此时就以传入的对象作为embedding\n",
    "        :param hidden_size: 模型中特征维度.\n",
    "        :param num_layers: 模型层数.\n",
    "        :param num_head: 模型中multi-head的head个数.\n",
    "        :param head_dim: 模型中multi-head中每个head特征维度.\n",
    "        :param max_len: 模型能接受的最大输入长度.\n",
    "        :param emb_dropout: 词嵌入的dropout概率.\n",
    "        :param dropout: 模型除词嵌入外的dropout概率.\n",
    "        \"\"\"\n",
    "        super(StarTransEnc, self).__init__()\n",
    "        self.embedding = get_embeddings(embed,padding_idx=0)\n",
    "        emb_dim = self.embedding.embedding_dim\n",
    "        self.emb_fc = nn.Linear(emb_dim, hidden_size)\n",
    "        # self.emb_drop = nn.Dropout(emb_dropout)\n",
    "        self.encoder = StarTransformer(hidden_size=hidden_size,\n",
    "                                       num_layers=num_layers,\n",
    "                                       num_head=num_head,\n",
    "                                       head_dim=head_dim,\n",
    "                                       dropout=dropout,\n",
    "                                       max_len=max_len)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        r\"\"\"\n",
    "        :param FloatTensor x: [batch, length, hidden] 输入的序列\n",
    "        :param ByteTensor mask: [batch, length] 输入序列的padding mask, 在没有内容(padding 部分) 为 0,\n",
    "            否则为 1\n",
    "        :return: [batch, length, hidden] 编码后的输出序列\n",
    "                [batch, hidden] 全局 relay 节点, 详见论文\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        x = self.emb_fc(x)\n",
    "        nodes, relay, relays_attns = self.encoder(x, mask)\n",
    "        return nodes, relay, relays_attns\n",
    "\n",
    "\n",
    "class _Cls(nn.Module):\n",
    "    def __init__(self, in_dim, num_cls, hid_dim, dropout=0.1):\n",
    "        super(_Cls, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_dim, hid_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hid_dim, num_cls),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.fc(x)\n",
    "        return h\n",
    "    \n",
    "class STSeqCls(nn.Module):\n",
    "    r\"\"\"\n",
    "    用于分类任务的Star-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed, num_cls=2,\n",
    "                 hidden_size=300,\n",
    "                 num_layers=1,\n",
    "                 num_head=9,\n",
    "                 head_dim=32,\n",
    "                 max_len=512,\n",
    "                 cls_hidden_size=600,\n",
    "                 emb_dropout=0.1,\n",
    "                 dropout=0.1):\n",
    "        r\"\"\"\n",
    "        \n",
    "        :param embed: 单词词典, 可以是 tuple, 包括(num_embedings, embedding_dim), 即\n",
    "            embedding的大小和每个词的维度. 也可以传入 nn.Embedding 对象, 此时就以传入的对象作为embedding\n",
    "        :param num_cls: 输出类别个数\n",
    "        :param hidden_size: 模型中特征维度. Default: 300\n",
    "        :param num_layers: 模型层数. Default: 4\n",
    "        :param num_head: 模型中multi-head的head个数. Default: 8\n",
    "        :param head_dim: 模型中multi-head中每个head特征维度. Default: 32\n",
    "        :param max_len: 模型能接受的最大输入长度. Default: 512\n",
    "        :param cls_hidden_size: 分类器隐层维度. Default: 600\n",
    "        :param emb_dropout: 词嵌入的dropout概率. Default: 0.1\n",
    "        :param dropout: 模型除词嵌入外的dropout概率. Default: 0.1\n",
    "        \"\"\"\n",
    "        super(STSeqCls, self).__init__()\n",
    "        self.enc = StarTransEnc(embed=embed,\n",
    "                                hidden_size=hidden_size,\n",
    "                                num_layers=num_layers,\n",
    "                                num_head=num_head,\n",
    "                                head_dim=head_dim,\n",
    "                                max_len=max_len,\n",
    "                                emb_dropout=emb_dropout,\n",
    "                                dropout=dropout)\n",
    "        self.cls = _Cls(hidden_size, num_cls, cls_hidden_size, dropout=dropout)\n",
    "\n",
    "    def forward(self, words, seq_len):\n",
    "        r\"\"\"\n",
    "        :param words: [batch, seq_len] 输入序列\n",
    "        :param seq_len: [batch,] 输入序列的长度\n",
    "        :return output: [batch, num_cls] 输出序列的分类的概率\n",
    "        \"\"\"\n",
    "        mask = seq_len_to_mask(seq_len,max_len=49).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        nodes, relay, relays_attns = self.enc(words, mask)\n",
    "        y = 0.5 * (relay + nodes.max(1)[0])\n",
    "        output = self.cls(y)  # [bsz, n_cls]\n",
    "        return output, relays_attns#, nodes_attns, relays_attns\n",
    "    \n",
    "\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d3d0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval = STSeqCls((21, 100), num_cls=2, hidden_size=300, num_layers=1, num_head=8, max_len=49,cls_hidden_size=600,dropout=0.1,head_dim=32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815e345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "def transfer(y_prob, threshold = 0.5):\n",
    "    return np.array([[0, 1][x > threshold] for x in y_prob])\n",
    "def eval_step_corrected(model, val_loader, use_cuda = False, save_ = False):\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_true_val_list, y_prob_val_list, dec_attns_val_list = [], [], []\n",
    "        for train_pep_inputs, train_pep_lens, train_labels in tqdm(val_loader):\n",
    "            '''\n",
    "            pep_inputs: [batch_size, pep_len]\n",
    "            hla_inputs: [batch_size, hla_len]\n",
    "            train_outputs: [batch_size, 2]\n",
    "            '''\n",
    "            train_pep_inputs, train_labels = train_pep_inputs.to(device), train_labels.to(device)\n",
    "            train_pep_lens = train_pep_lens.to(device)\n",
    "            val_outputs, val_dec_self_attns = model(train_pep_inputs, train_pep_lens)\n",
    "\n",
    "            y_true_val = train_labels.cpu().numpy()\n",
    "            y_prob_val = nn.Softmax(dim = 1)(val_outputs)[:, 1].cpu().detach().numpy()\n",
    "\n",
    "            y_true_val_list.extend(y_true_val)\n",
    "            y_prob_val_list.extend(y_prob_val)\n",
    "            \n",
    "            if save_:\n",
    "                dec_attns_val_list.extend(val_dec_self_attns[0][:, :, :, 34:]) # 只要（34,15）行HLA，列peptide\n",
    "                \n",
    "#         assert (labels.numpy() == y_true_val_list).all()    \n",
    "        y_pred_val_list = transfer(y_prob_val_list, threshold)\n",
    "        ys_val = (y_true_val_list, y_pred_val_list, y_prob_val_list)\n",
    "#         metrics_val = performances(y_true_val_list, y_pred_val_list, y_prob_val_list, print_ = True)\n",
    "        \n",
    "        if save_: \n",
    "            return ys_val, dec_attns_val_list\n",
    "        else:\n",
    "            return ys_val, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d078a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_ = 'all_corrected'\n",
    "\n",
    "# model_file = 'model_layer1_multihead9_fold4.pkl'\n",
    "\n",
    "save_ = True\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "_, attn_res = eval_step_corrected(model_eval, train_loader, use_cuda, save_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b4c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_sumhead_peplength_pepposition(data, attn_data, label = None):\n",
    "    SUM_length_head_dict = {}\n",
    "    for l in range(8, 15):\n",
    "        print('Length = ', str(l))\n",
    "        SUM_length_head_dict[l] = []\n",
    "        \n",
    "        if label == None:\n",
    "            length_index = np.array(data[data.length == l].index)\n",
    "        elif label == 1:\n",
    "            length_index = np.array(data[data.label == 1][data.length == l].index)\n",
    "        elif label == 0:\n",
    "            length_index = np.array(data[data.label == 0][data.length == l].index)\n",
    "            \n",
    "        length_data_num = len(length_index)\n",
    "        print(length_data_num, length_index)\n",
    "\n",
    "        for head in trange(8):\n",
    "            idx_0 = length_index[0]\n",
    "            temp_length_head = deepcopy(nn.Softmax(dim = -1)(attn_data[idx_0][head][:,:,:l].float())) # Shape = (34, length), 行是HLA，列是peptide，由行查列\n",
    "\n",
    "            for idx in length_index[1:]:\n",
    "                temp_length_head += nn.Softmax(dim = -1)(attn_data[idx][head][:,:,:l].float())\n",
    "\n",
    "            temp_length_head = np.array(nn.Softmax(dim = -1)(temp_length_head.sum(axis = 1)[0]).cpu()) # 把这一列的数据相加，shape = （length，）\n",
    "            print(temp_length_head.shape)\n",
    "            SUM_length_head_dict[l].append(temp_length_head)\n",
    "            \n",
    "    #############################\n",
    "    SUM_length_head_sum = []\n",
    "    for l in range(8, 15):\n",
    "        print(l)\n",
    "        temp = pd.DataFrame(SUM_length_head_dict[l], columns = range(1, l+1)).round(4)\n",
    "        temp.loc['sum'] = temp.sum(axis = 0)\n",
    "        SUM_length_head_sum.append(list(temp.loc['sum']))\n",
    "        print(l, temp.loc['sum'].sort_values(ascending = False).index)\n",
    "        \n",
    "    return SUM_length_head_dict, SUM_length_head_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正样本\n",
    "positive_sum_peplength_pepposition, positive_sum_peplength_pepposition_headsum = attn_sumhead_peplength_pepposition(df_data, attn_res, label = 1)\n",
    "positive_sum_peplength_pepposition_headsum\n",
    "# 负样本\n",
    "negative_sum_peplength_pepposition, negative_sum_peplength_pepposition_headsum = attn_sumhead_peplength_pepposition(df_data, attn_res, label = 0)\n",
    "negative_sum_peplength_pepposition_headsum\n",
    "# 全部样本\n",
    "sum_peplength_pepposition, sum_peplength_pepposition_headsum = attn_sumhead_peplength_pepposition(df_data, attn_res, label = 0)\n",
    "sum_peplength_pepposition_headsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554bed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a80c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=3,#start_cell=\"bottom-left\", # 'bottom-left', 'top-left\n",
    "                    subplot_titles=[\"All Samples\",\"Positive Samples\",\"Negative Samples\"],shared_yaxes=True)  # 1行2列\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=pd.DataFrame(sum_peplength_pepposition_headsum),colorscale='teal',\n",
    "                   x=['1', '2', '3', '4', '5','6','7','8','9','10','11', '12', '13', '14'],\n",
    "                   y=['8', '9', '10','11', '12', '13','14'],coloraxis='coloraxis',\n",
    "                   hoverongaps = False),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=pd.DataFrame(positive_sum_peplength_pepposition_headsum),colorscale='teal',\n",
    "                   x=['1', '2', '3', '4', '5','6','7','8','9','10','11', '12', '13', '14'],\n",
    "                   y=['8', '9', '10','11', '12', '13','14'],coloraxis='coloraxis',\n",
    "                   hoverongaps = False),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=pd.DataFrame(negative_sum_peplength_pepposition_headsum),colorscale='teal',\n",
    "                   x=['1', '2', '3', '4', '5','6','7','8','9','10','11', '12', '13', '14'],\n",
    "                   y=['8', '9', '10','11', '12', '13','14'],coloraxis='coloraxis',\n",
    "                   hoverongaps = False),\n",
    "    row=1, col=3\n",
    ")\n",
    "fig.update_layout(width=1000,height=350,\n",
    "                      paper_bgcolor='rgba(0,0,0,0)',\n",
    "    plot_bgcolor='rgba(0,0,0,0)',\n",
    "        font=dict(\n",
    "        family=\"black\",\n",
    "        size=13))\n",
    "fig.update_xaxes(tickangle=0, tickfont=dict(family='black', size=12))\n",
    "fig.update_xaxes(title='peptide position')\n",
    "fig.update_yaxes(title_text=\"peptide length\", row=1, col=1)\n",
    "fig.update_coloraxes(colorbar_thickness=5)\n",
    "fig.update_coloraxes(colorscale='teal')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07eb55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "iplot(fig, image='svg', filename='attention', image_width=1000, image_height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008e751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_HLA_length_aatype_position_num(data, attn_data, hla = 'HLA-A*11:01', label = None, length = 9, show_num = False):\n",
    "    aatype_position = dict()\n",
    "    if label == None:\n",
    "        length_index = np.array(data[data.length == length][data.HLA == hla].index)\n",
    "    else:\n",
    "        length_index = np.array(data[data.length == length][data.HLA == hla][data.label == label].index)\n",
    "\n",
    "    length_data_num = len(length_index)\n",
    "    print(length_data_num)\n",
    "\n",
    "    for head in trange(8):\n",
    "        for idx in length_index:\n",
    "            temp_peptide = data.iloc[idx].peptide\n",
    "            temp_length_head = deepcopy(nn.Softmax(dim=-1)(attn_data[idx][head][:, :length].float())) # Shape = (34, 9), 行是HLA，列是peptide，由行查列\n",
    "            temp_length_head = nn.Softmax(dim=-1)(temp_length_head.sum(axis = 0)) # 把这一列的数据相加，shape = （9，）\n",
    "\n",
    "            for i, aa in enumerate(temp_peptide): \n",
    "                aatype_position.setdefault(aa, {})\n",
    "                aatype_position[aa].setdefault(i, 0)\n",
    "                aatype_position[aa][i] += temp_length_head[i] \n",
    "    \n",
    "    if show_num:\n",
    "        aatype_position_num = dict()\n",
    "        for idx in length_index:\n",
    "            temp_peptide = data.iloc[idx].peptide\n",
    "            for i, aa in enumerate(temp_peptide):\n",
    "                aatype_position_num.setdefault(aa, {})\n",
    "                aatype_position_num[aa].setdefault(i, 0)\n",
    "                aatype_position_num[aa][i] += 1\n",
    "             \n",
    "        return aatype_position, aatype_position_num\n",
    "    else:\n",
    "        return aatype_position\n",
    "def attn_HLA_length_aatype_position_pd(HLA_length_aatype_position, length = 9, softmax = True, unsoftmax = True):\n",
    "        \n",
    "    HLA_length_aatype_position_pd = np.zeros((20, length))\n",
    "    \n",
    "    aai, aa_indexs = 0, []\n",
    "    for aa, aa_posi in HLA_length_aatype_position.items():\n",
    "        aa_indexs.append(aa)\n",
    "        for posi, v in aa_posi.items():\n",
    "            HLA_length_aatype_position_pd[aai, posi] = v\n",
    "        aai += 1\n",
    "    \n",
    "    if len(aa_indexs) != 20: \n",
    "        aatype_sorts = list('YATVLDEGRHIWQKMFNSPC')\n",
    "        abscent_aa = list(set(aatype_sorts).difference(set(aa_indexs)))\n",
    "        aa_indexs += abscent_aa\n",
    "    \n",
    "    if softmax and not unsoftmax: \n",
    "        HLA_length_aatype_position_softmax_pd = deepcopy(nn.Softmax(dim = -1)(torch.Tensor(HLA_length_aatype_position_pd)))\n",
    "        HLA_length_aatype_position_softmax_pd = np.array(HLA_length_aatype_position_softmax_pd)\n",
    "        HLA_length_aatype_position_softmax_pd = pd.DataFrame(HLA_length_aatype_position_softmax_pd, \n",
    "                                                             index = aa_indexs, columns = range(1, length + 1))\n",
    "        return HLA_length_aatype_position_softmax_pd\n",
    "    \n",
    "    elif unsoftmax and not softmax:\n",
    "        HLA_length_aatype_position_unsoftmax_pd = pd.DataFrame(HLA_length_aatype_position_pd,\n",
    "                                                               index = aa_indexs, columns = range(1, length + 1))\n",
    "        return HLA_length_aatype_position_unsoftmax_pd\n",
    "    \n",
    "    elif softmax and unsoftmax:\n",
    "        HLA_length_aatype_position_softmax_pd = deepcopy(nn.Softmax(dim = -1)(torch.Tensor(HLA_length_aatype_position_pd)))\n",
    "        HLA_length_aatype_position_softmax_pd = np.array(HLA_length_aatype_position_softmax_pd)\n",
    "        HLA_length_aatype_position_softmax_pd = pd.DataFrame(HLA_length_aatype_position_softmax_pd, \n",
    "                                                             index = aa_indexs, columns = range(1, length + 1))\n",
    "        \n",
    "        HLA_length_aatype_position_unsoftmax_pd = pd.DataFrame(HLA_length_aatype_position_pd,\n",
    "                                                               index = aa_indexs, columns = range(1, length + 1))\n",
    "        return HLA_length_aatype_position_softmax_pd, HLA_length_aatype_position_unsoftmax_pd\n",
    "def draw_hla_length_aatype_position(data, attn_data, hla = 'HLA-B*27:05', label = None, length = 9, \n",
    "                                    show = True, softmax = True, unsoftmax = True):\n",
    "    \n",
    "    HLA_length_aatype_position = attn_HLA_length_aatype_position_num(data, attn_data, hla, label, length, show_num = False)\n",
    "    print(HLA_length_aatype_position)\n",
    "    \n",
    "    if softmax and unsoftmax:\n",
    "        HLA_length_aatype_position_softmax_pd, HLA_length_aatype_position_unsoftmax_pd = attn_HLA_length_aatype_position_pd(\n",
    "                                                                                     HLA_length_aatype_position, \n",
    "                                                                                     length, \n",
    "                                                                                     softmax,\n",
    "                                                                                     unsoftmax)\n",
    "        HLA_length_aatype_position_softmax_pd = sort_aatype(HLA_length_aatype_position_softmax_pd)\n",
    "        HLA_length_aatype_position_unsoftmax_pd = sort_aatype(HLA_length_aatype_position_unsoftmax_pd)\n",
    "        \n",
    "        if show:\n",
    "            fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (10, 8))\n",
    "            sns.heatmap(HLA_length_aatype_position_softmax_pd,\n",
    "                        ax = axes[0], cmap = 'YlGn', square = True)\n",
    "\n",
    "            sns.heatmap(HLA_length_aatype_position_unsoftmax_pd,\n",
    "                        ax = axes[1], cmap = 'YlGn', square = True)\n",
    "\n",
    "            axes[0].set_title(hla + ' Softmax Normalization')\n",
    "            axes[1].set_title(hla + ' UnNormalization')\n",
    "            plt.show()\n",
    "\n",
    "        return HLA_length_aatype_position_softmax_pd, HLA_length_aatype_position_unsoftmax_pd\n",
    "    \n",
    "    else:\n",
    "        HLA_length_aatype_position_pd = attn_HLA_length_aatype_position_pd(HLA_length_aatype_position, \n",
    "                                                                           length, \n",
    "                                                                           softmax,\n",
    "                                                                           unsoftmax)\n",
    "        HLA_length_aatype_position_pd = sort_aatype(HLA_length_aatype_position_pd)\n",
    "        return HLA_length_aatype_position_pd\n",
    "def sort_aatype(df):\n",
    "    aatype_sorts = list('YATVLDEGRHIWQKMFNSPC')\n",
    "    df.reset_index(inplace = True)\n",
    "    df['index'] = df['index'].astype('category')\n",
    "    df['index'].cat.reorder_categories(aatype_sorts, inplace=True)\n",
    "    df.sort_values('index', inplace=True)\n",
    "    df.rename(columns = {'index':''}, inplace = True)\n",
    "    df = df.set_index('')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f989126",
   "metadata": {},
   "outputs": [],
   "source": [
    "A0101_length9_positive_aatype_position_unsoftmax_pd = draw_hla_length_aatype_position(df_data, attn_res, 'HLA-A01:01', label = 1, length = 9, show = False, softmax = False, unsoftmax = True)\n",
    "A0201_length9_positive_aatype_position_unsoftmax_pd = draw_hla_length_aatype_position(df_data, attn_res, 'HLA-A02:01', label = 1, length = 9, show = False, softmax = False, unsoftmax = True)\n",
    "A0301_length9_positive_aatype_position_unsoftmax_pd = draw_hla_length_aatype_position(df_data, attn_res, 'HLA-A03:01', label = 1, length = 9, show = False, softmax = False, unsoftmax = True)\n",
    "B0702_length9_positive_aatype_position_unsoftmax_pd = draw_hla_length_aatype_position(df_data, attn_res, 'HLA-B07:02', label = 1, length = 9, show = False, softmax = False, unsoftmax = True)\n",
    "B2705_length9_positive_aatype_position_unsoftmax_pd = draw_hla_length_aatype_position(df_data, attn_res, 'HLA-B27:05', label = 1, length = 9, show = False, softmax = False, unsoftmax = True)\n",
    "B5701_length9_positive_aatype_position_unsoftmax_pd = draw_hla_length_aatype_position(df_data, attn_res, 'HLA-B57:01', label = 1, length = 9, show = False, softmax = False, unsoftmax = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55308137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两个基本参数：设置行、列\n",
    "fig = make_subplots(rows=1, cols=6,horizontal_spacing=0.02,x_title='Peptide position',#start_cell=\"bottom-left\", # 'bottom-left', 'top-left\n",
    "                    subplot_titles=[\"HLA-A01:01\",\"HLA-A02:01\",\"HLA-A03:01\",'HLA-B07:02','HLA-B27:05','HLA-B57:01'],shared_yaxes=True,shared_xaxes=True)  # 1行2列\n",
    "\n",
    "# 添加两个数据轨迹，构成两个图形\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=pd.DataFrame(A0101_length9_positive_aatype_position_unsoftmax_pd),colorscale='teal',\n",
    "                   x=['1', '2', '3', '4', '5','6','7','8','9'],\n",
    "                   y=['Y', 'A', 'T', 'V', 'L','D','E','G','R', 'H','I', 'W', 'Q','K', 'M','F', 'N', 'S','P','C'],coloraxis='coloraxis',\n",
    "                   hoverongaps = False),\n",
    "    row=1, col=1  # 第一行第一列\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=pd.DataFrame(A0201_length9_positive_aatype_position_unsoftmax_pd),colorscale='teal',\n",
    "                   x=['1', '2', '3', '4', '5','6','7','8','9'],\n",
    "                   y=['Y', 'A', 'T', 'V', 'L','D','E','G','R', 'H','I', 'W', 'Q','K', 'M','F', 'N', 'S','P','C'],coloraxis='coloraxis',\n",
    "                   hoverongaps = False),\n",
    "    row=1, col=2  # 第一行第二列\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=pd.DataFrame(A0301_length9_positive_aatype_position_unsoftmax_pd),colorscale='teal',\n",
    "                    x=['1', '2', '3', '4', '5','6','7','8','9'],\n",
    "                   y=['Y', 'A', 'T', 'V', 'L','D','E','G','R', 'H','I', 'W', 'Q','K', 'M','F', 'N', 'S','P','C'],coloraxis='coloraxis',\n",
    "                   hoverongaps = False),\n",
    "    row=1, col=3  # 第一行第二列\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=pd.DataFrame(B0702_length9_positive_aatype_position_unsoftmax_pd),colorscale='teal',\n",
    "                    x=['1', '2', '3', '4', '5','6','7','8','9'],\n",
    "                   y=['Y', 'A', 'T', 'V', 'L','D','E','G','R', 'H','I', 'W', 'Q','K', 'M','F', 'N', 'S','P','C'],coloraxis='coloraxis',\n",
    "                   hoverongaps = False),\n",
    "    row=1, col=4  # 第一行第二列\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=pd.DataFrame(B2705_length9_positive_aatype_position_unsoftmax_pd),colorscale='teal',\n",
    "                    x=['1', '2', '3', '4', '5','6','7','8','9'],\n",
    "                   y=['Y', 'A', 'T', 'V', 'L','D','E','G','R', 'H','I', 'W', 'Q','K', 'M','F', 'N', 'S','P','C'],coloraxis='coloraxis',\n",
    "                   hoverongaps = False),\n",
    "    row=1, col=5  # 第一行第二列\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=pd.DataFrame(B5701_length9_positive_aatype_position_unsoftmax_pd),colorscale='teal',\n",
    "                    x=['1', '2', '3', '4', '5','6','7','8','9'],\n",
    "                   y=['Y', 'A', 'T', 'V', 'L','D','E','G','R', 'H','I', 'W', 'Q','K', 'M','F', 'N', 'S','P','C'],coloraxis='coloraxis',\n",
    "                   hoverongaps = False),\n",
    "    row=1, col=6  # 第一行第二列\n",
    ")\n",
    "fig.update_layout(width=1000,height=500,\n",
    "                      paper_bgcolor='rgba(0,0,0,0)',\n",
    "    plot_bgcolor='rgba(0,0,0,0)',\n",
    "        font=dict(\n",
    "        family=\"black\",\n",
    "        size=13))\n",
    "fig.update_xaxes(tickangle=0, tickfont=dict(family='black', size=12))\n",
    "# fig.update_xaxes(title='Peptide position', row=1, col=3)\n",
    "fig.update_yaxes(title_text=\"Amino acid type\", row=1, col=1)\n",
    "fig.update_coloraxes(colorbar_thickness=8)\n",
    "fig.update_coloraxes(colorscale='teal')\n",
    "# fig.update_yaxes(title='peptide length')\n",
    "# # 设置图形的宽高和标题\n",
    "# fig.update_layout(height=300, \n",
    "#                   width=800, \n",
    "#                   title_text=\"子图制作\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910417e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iplot(fig, image='svg', filename='attention', image_width=1000, image_height=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
